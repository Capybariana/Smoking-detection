import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import accuracy_score
import random
import numpy as np
import os

from model import TemporalTransformer, KeypointSequenceDataset

# ======= ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ =======
video_path = 'data/video.mp4'
csv_path = 'data/annotations.csv'
seq_len = 10
batch_size = 16
learning_rate = 1e-4
num_epochs = 100
save_path = './best_model_100.pth'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ======= Ğ¤Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµĞ¼ Ğ·ĞµÑ€Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ =======
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
set_seed()

# ======= Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ =======
full_dataset = KeypointSequenceDataset(video_path=video_path, csv_path=csv_path, seq_len=seq_len)

# 80% train, 20% val
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# ======= ĞœĞ¾Ğ´ĞµĞ»ÑŒ =======
model = TemporalTransformer(
    seq_len=seq_len,
    num_joints=17,
    joint_dim=3,
    num_classes=2,
    d_model=128,
    nhead=4,
    num_layers=2
).to(device)

# ======= ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ =======
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# ======= Ğ¨ĞµĞ´ÑƒĞ»ĞµÑ€ (ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ LR, ĞµÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ) =======
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)

best_acc = 0.0


# ======= Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾Ñ…Ğµ =======
def run_epoch(model, dataloader, criterion, optimizer=None, train=True):
    model.train() if train else model.eval()
    epoch_loss = 0.0
    all_preds, all_labels = [], []

    with torch.set_grad_enabled(train):
        for seq, label in dataloader:
            seq, label = seq.to(device), label.to(device)

            if train:
                optimizer.zero_grad()

            outputs = model(seq)
            loss = criterion(outputs, label)

            if train:
                loss.backward()
                optimizer.step()

            epoch_loss += loss.item()
            _, preds = torch.max(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(label.cpu().numpy())

    avg_loss = epoch_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    return avg_loss, accuracy


# ======= ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» =======
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, save_path):
    global best_acc
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")

        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, train=True)
        val_loss, val_acc = run_epoch(model, val_loader, criterion, train=False)

        scheduler.step(val_acc)  # Ğ˜Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼ learning rate Ğ¿Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ

        print(f"Train     Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}")
        print(f"Val       Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}")
        print(f"LR        Now : {optimizer.param_groups[0]['lr']:.6f}")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), save_path)
            print(f"âœ… New best model saved with val_acc = {best_acc:.4f}")


# ======= Ğ—Ğ°Ğ¿ÑƒÑĞº =======
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, save_path)
print(f"\nğŸ¯ Training complete. Best validation accuracy: {best_acc:.4f}")
